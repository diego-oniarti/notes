\documentclass{article}
\usepackage[top=2cm,bottom=2cm,left=2cm,right=3.5cm]{geometry}

\usepackage[utf8]{inputenc}
\input{../common}
\usepackage[parfill]{parskip}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    % filecolor=magenta,      
    urlcolor=blue,
    % pdftitle={Overleaf Example},
    % pdfpagemode=FullScreen,
}

\usepackage{graphicx}
\usepackage{algorithm2e}

\title{
    GPU computing homework\\
\large CSR group}
\author{Diego Oniarti - 257835 \\ diego.oniarti@studenti.unitn.it}
\date{2024-2025}

\begin{document}

\maketitle

\section{Introduction}
The problem at hand is that of performing a \textit{Sparse-Matrix Dense-Vector Multiplication} and implementing different optimizations through the use of the GPU.

The matrix will be read from a \texttt{.mtx} file and be converted into CSR format before being used for the multiplication. The time used for the conversion in CSR form will not be taken into consideration when measuring the time taken by the multiplications.

The repository with the code can be found at \url{https://github.com/diego-oniarti/GPU_homework}.

\subsection{Abstractions}
In the code the type of the elements in the matrix and the vector will be \texttt{data\_t}, defined in a macro, and the matrix will be passed to the functions as a struct of this form:
\begin{verbatim}
typedef struct {
    data_t *vals;   // Vector containing the nonzero values
    int *xs;        // Vector containing the x indices of the nonzero elements
    int *ys;        // Vector containing the row pointer
    int nvals;      // Number of nonzero values
    int nrows;      // Number of rows
    int ncols;      // Number of columns
} MAT_CSR;
\end{verbatim}

\section{CPU algorithm}
I started with a sequential implementation on the CPU as a baseline. The result of this implementation will be used to assess the correctness of the following ones.
\SetKwComment{Comment}{//}{}
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}
\newcommand\commentFont[1]{\ttfamily\textcolor{blue}{#1}}
\SetCommentSty{commentFont}
\begin{algorithm}[ht]
    \label{algo:CPU}
    \caption{CPU implementation}
    \Input{MAT\_CSR *mat, data\_t *vec}
    \Output{data\_t* result}
    \Comment{Iterate over the rows}
    \For{(int y=0; y$<$mat$\to$ nrows; y++)}{
        data\_t row\_acc = 0\;
        \Comment{Iterate over the elements in the row}
        \For{(int i=mat$\to$ys[y]; i$<$mat$\to$ys[y+1]; i++)}{
            int x = mat$\to$xs[i]\;
            row\_acc += mat$\to$vals[i] * vec[x]\;
        }
        result[y] = row\_acc\;
    }
\end{algorithm}
\begin{callout}{Remark}
    In this and other pseudocodes, trivial allocations and freeing of memory will be omitted for the sake of clarity and to center focus on the algorithm.
\end{callout}
The values in the arrays \texttt{ys}, \texttt{xs}, \texttt{vals}, and \texttt{ret} are all accessed sequentially, making good use of caching.\\
The slight exception to this is \texttt{ys}, where each element is read along with its successor (in the halt check of the inner loop). The values being accessed are however close together, so locality of reference is still taken advantage of.

The worst use of cache in this access pattern happens when accessing the vector, since values are read randomly from it.

\section{GPU algorithms}
I implemented 4 different kernels to run on the GPU, each one improving in some aspect on the previous ones.

\subsection{Thread Per Row}
This first implementation mirrors the one done on the CPU, but instead of rows being accessed sequentially, a thread is generated to handle each of them individually.
\begin{algorithm}[ht]
    \caption{Thread Per Row}
    \Input{data\_t *ret, *vals, *vec \\int rows, *xs, *ys}
    int tid = blockIdx.x*blockDim.x + threadIdx.x;
    \If {(tid $<$ rows)} {
        \Comment{The elements in the row are summed linearly and sequentially}
        data\_t acc = 0\;
        \For {(int i=ys[tid]; i$<$ys[tid+1]; i++)} {
            acc += vals[i] * vec[xs[i]]\;
        }
        ret[tid] = acc\;
    }
\end{algorithm}
This implementation retains the locality of access advantages from its fully sequential counterpart, but its made much faster through parallelization.

\subsection{Warp Per Row}
The logical progression from the algorithm using one thread per row would be to use one thread per nonzero value. Using this solution it is quite easy to get the product of each element in the matrix with the corresponding one in the vector.\\
When summing the elements in a row, however, the synchronization is much harder. Having one thread sum up all the elements in the row would be inefficient, while doing a reduction in parallel requires the thread to synchronize possible across blocks.

My solution to this was to dedicate a whole warp of threads to each row. This way the synchronization is much easier, and the performance is still improved from the previous implementation since more threads are used.

To allow the sum of the elements in a row to be performed by multiple threads, the individual values are stored into a buffer. Parts of the buffer are then reduced and the result is put in the return array.

\begin{algorithm}[ht]
    \caption{Warp Per Row}
    \Input{data\_t *vals, *vec, *ret, *buffer,\\
    int *xs, *ys, cols, rows}
    int tid = blockIdx.x*blockDim.x + threadIdx.x\;
    int wid = threadIdx.x / 32\;
    int friend\_id = threadIdx.x \% 32 \Comment*[r]{Thread's place in the warp}
    int row = blockIdx.x * (blockDim.x / 32) + wid\;

    buffer[tid] = 0 \Comment*{Set to zero to avoid breaking the reduction}
    \If {(row $<$ rows)} {
        int start = ys[row]\;
        int end = ys[row+1]\;

        \Comment{Loop with stride equal to the warp size\\Needed in case there are more elements in a row than threads in a warp}
        data\_t sum = 0\;
        \For {(int i=start+friend\_id; i$<$end; i+=32)} {
            sum += vals[i] * vec[xs[i]]\;
        }
        buffer[tid] = sum\;
    }

    \Comment{Perform the reduction over the warp}
    \For {(int s=1; s$<$32; s$<<=$1)} {
        \_\_syncthreads() \Comment*[r]{Wait for the previous iteration to be over}
        \If {((tid $\&$ ((s$<<$1)-1)) == 0)} {
            buffer[tid] += buffer[tid+s]\;
        }
    }

    \Comment{The first thread in the warp is tasked with moving the result to the output}
    \If {(friend\_id==0 $\&\&$ row$<$rows)} {
        ret[row] = buffer[tid];
    }
\end{algorithm}

\subsection{Adding shared memory}
The final improvement I made to the algorithm was that of removing the buffer used for the reduction and perform that in shared memory instead.

% \begin{callout}[red]{}
%     A different approach would be that of targeting temporal locality. The 
% \end{callout}

\section{Mode of testing}
\subsection{Output validity}
As mention prior, the result of each execution is compared to that of the sequential CPU implementation. This had shown no errors for Algorithm \ref{algo:CPU}.
Errors in the result occur in the last two algorithms. I do however have good reason to attribute this error to the imprecision of floating point operations in C and to the fact addition is not commutative.\\
The proof I have is:
\begin{enumerate}
    \item The maximum error decreases when switching \texttt{data\_t} from \texttt{float} to \texttt{double}
    \item The error completely vanishes if the matrix only contains integers
    \item The error is consistent across runs and valgrind confirms no read from dirty memory occurs
    \item The error is always very small, in the order of $10^{-16}$
\end{enumerate}
\subsection{Timing}
For the CPU implementation I've used the \texttt{gettimeofday} function to get the timing of the execution, while for the GPU kernels I made use of CUDA events.

Each algorithm has been tested with different block sizes, ranging from 32 to 1024 (the maximum allowed by the GPU).

Moreover, each of these runs has been repeated 13 times: 3 pre-runs and 10 actual runs. From the latter 10 runs I got the average time and the standard derivation in the values.

\subsection{Datasets}
The algorithms have been tested on 4 different matrices, while the vector has always been filled with ones.\\
One of the matrices is generated on the spot, each cell having a $1\%$ chance of being a random value from 1 to 10. The others have been taken from the \href{https://sparse.tamu.edu/}{UF sparse matrix collection}.
\begin{center}
    \begin{tabular}{l|r|r|rl|l}
        name & rows & columns & nonzeros & (\%) & type \\
        \hline
        lp\_ganges\footnotemark & 1309 & 1706 & 6937 & 0.3106\% & real \\
        delaunay\_n23\footnotemark & 8388608 & 8388608 & 50331568 & 7.152546e-5\% & binary \\
        Stanford\_Berkeley\footnotemark & 683446 & 683446 & 7583376 & 0.001624\% & binary \\
        custom & 300000 & 200000 & - & 1\% & real \\
    \end{tabular}
\end{center}
\footnotetext[1]{\url{https://www.cise.ufl.edu/research/sparse/matrices/LPnetlib/lp_ganges}}
\footnotetext[2]{\url{https://www.cise.ufl.edu/research/sparse/matrices/DIMACS10/delaunay_n23}}
\footnotetext[3]{\url{https://www.cise.ufl.edu/research/sparse/matrices/Kamvar/Stanford_Berkeley}}

\section{Results}

\end{document}
